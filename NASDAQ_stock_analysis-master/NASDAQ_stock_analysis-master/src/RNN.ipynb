{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSRscAYN0TE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a37537-de72-44ca-fc51-5c4664812821"
      },
      "source": [
        "#mounting for using dataset and saving work\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdTflvtruX_q"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtf8JRtrIHsN"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WZqOMx72Tgy"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8tNQdgUjdrz"
      },
      "source": [
        "!pip install TA-Lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1vVKL_fjuDA"
      },
      "source": [
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZJ-IUE40nxb"
      },
      "source": [
        "  #All the stock market abbrevations used for companies   \n",
        "stocks = {'AXP':'American Express Financial services',\n",
        "'AAPL':'Apple Technology',\n",
        "'AMZN':'Amazon.com E-commerce',\n",
        "'ADBE':'Adobe Computer software',\n",
        "'ADS':'Alliance Data Systems Corporation',\n",
        "'BA':'Boeing Aerospace',\n",
        "'BAC':'Bank of America Investment banking',\n",
        "'CAT':'Caterpillar Inc. Construction machinery and equipment',\n",
        "'CVX':'Chevron Corporation Oil industry',\n",
        "'CSCO':'Cisco Systems Networking hardware',\n",
        "'COST':'Costco Retail',\n",
        "'DIS':'The Walt Disney Company Mass media',\n",
        "'FB':'Facebook Social Media',\n",
        "'GE':'General Electric Multinational conglomerate',\n",
        "'GS':'Goldman Sachs Investment banking',\n",
        "'GOOGL':'Alphabet Inc. Multinational conglomerate',\n",
        "'HD':'The Home Depot Home improvement',\n",
        "'HDB':'HDFC Bank Financial services',\n",
        "'HOG':'Harley:Davidson motorcycle manufacturer',\n",
        "'IBM':'IBM Computer hardware',\n",
        "'INTC':'Intel Semiconductor',\n",
        "'INFY':'Infosys Information technology consulting',\n",
        "'JNJ':'Johnson & Johnson Medical device',\n",
        "'JPM':'JPMorgan Chase Investment banking',\n",
        "'JBHT':'J.B. Hunt Transportation and Logistics',\n",
        "'KO':'The Coca:Cola Company Beverages',\n",
        "'KEX':'Kirby Corporation Transportation',\n",
        "'MCD':\"McDonald's Fast food\",\n",
        "'MSFT':'Microsoft Corporation Technology',\n",
        "'MMM':'3M Multinational conglomerate',\n",
        "'MRK':'Merck & Co. Pharmaceutical',\n",
        "'NFLX':'Netflix Production',\n",
        "'NKE':'Nike Footwear manufacturing',\n",
        "'NVDA':'Nvidia Computer Hardware and Software', \n",
        "'PFE':'Pfizer Pharmaceutical',\n",
        "'PG':'Procter & Gamble Consumer Goods',\n",
        "'PEP':'PepsiCo Food',\n",
        "'REGN':'Regeneron Pharmaceuticals Biotechnology',\n",
        "'SBUX':'Starbucks Coffeehouse',\n",
        "'TRV':'The Travelers Companies Insurance',\n",
        "'TGT':'Target Corporation Retail',\n",
        "'T':'AT&T Telecommunications',\n",
        "'UTX':'Raytheon Technologies Corporation Aircraft manufacturing',\n",
        "'UNH':'UnitedHealth Group Managed care',\n",
        "'UPS':'United Parcel Service Logistics',\n",
        "'UNP':'Union Pacific Corporation Transport',\n",
        "'VZ':'Verizon Communications Telecommunications',\n",
        "'WMT':'Walmart Retail',\n",
        "'XOM':'ExxonMobil multinational oil and gas',\n",
        "'YUM':'Yum! Brands Fast food'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_wCd7Bu0jLj"
      },
      "source": [
        "# Read dataset\n",
        "path = 'drive/My Drive/data/'+'BA'+'_2006-01-01_to_2020-09-30.csv'\n",
        "df = pd.read_csv(path, index_col='Date', parse_dates=[\"Date\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mbkRkChzNUR"
      },
      "source": [
        "  #splitting training and testing dataset according to 2008 recession and 2020 pandemic\n",
        "train_1 = df[:'2008-05'].iloc[:,0:1].values\n",
        "test_1 = df['2008-05':'2010'].iloc[:,0:1].values\n",
        "train_2 = df['2018':'2019'].iloc[:,0:1].values\n",
        "test_2 = df['2020':].iloc[:,0:1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpiQyW9h0L78"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "  #transform features by scaling each feature to the given range\n",
        "scaler = MinMaxScaler(feature_range=(0,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAw7KHbK0Nzr"
      },
      "source": [
        "  #fitting model by maximum likelihood\n",
        "train_1 = scaler.fit_transform(train_1)\n",
        "train_2 = scaler.fit_transform(train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZT1rWZFQGG"
      },
      "source": [
        "  #inserting trainning values according to their respective lists\n",
        "x_train_1 = list()\n",
        "y_train_1 = list()\n",
        "\n",
        "x_train_2 = list()\n",
        "y_train_2 = list()\n",
        "\n",
        "for i in range(100,len(train_1)):\n",
        "    x_train_1.append(train_1[i-100:i, 0])\n",
        "    y_train_1.append(train_1[i,0])\n",
        "\n",
        "x_train_1, y_train_1 = np.array(x_train_1), np.array(y_train_1)\n",
        "\n",
        "for i in range(100,len(train_2)):\n",
        "    x_train_2.append(train_2[i-100:i, 0])\n",
        "    y_train_2.append(train_2[i,0])\n",
        "\n",
        "x_train_2, y_train_2 = np.array(x_train_2), np.array(y_train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyU58CEHSJ0"
      },
      "source": [
        "  #reshaping the dataset to fit properly for lstm\n",
        "x_train_1 = np.reshape(x_train_1, (x_train_1.shape[0], x_train_1.shape[1], 1))\n",
        "x_train_2 = np.reshape(x_train_2, (x_train_2.shape[0], x_train_2.shape[1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEHQN-fjHmWE"
      },
      "source": [
        "#training lstm/RNN model for train_1 (2008 recession)\n",
        "regressor_1 = Sequential()\n",
        "\n",
        "regressor_1.add(LSTM(units=60, return_sequences=True, input_shape=(x_train_1.shape[1],1)))\n",
        "regressor_1.add(Dropout(0.2))\n",
        "\n",
        "regressor_1.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_1.add(Dropout(0.2))\n",
        "\n",
        "regressor_1.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_1.add(Dropout(0.2))\n",
        "\n",
        "regressor_1.add(LSTM(units=50))\n",
        "regressor_1.add(Dropout(0.2))\n",
        "\n",
        "regressor_1.add(Dense(units=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GB46O5jIgud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d414bbc5-7795-472f-fd87-409b22af524f"
      },
      "source": [
        "  #calculating loss using mean squared error and optimizing the model accordingly for train_1 (2008 recession)\n",
        "regressor_1.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "regressor_1.fit(x_train_1, y_train_1, epochs=7, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 0.1011\n",
            "Epoch 2/7\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0301\n",
            "Epoch 3/7\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 0.0296\n",
            "Epoch 4/7\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 0.0245\n",
            "Epoch 5/7\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 0.0293\n",
            "Epoch 6/7\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 0.0212\n",
            "Epoch 7/7\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 0.0247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f079ec13f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcMdlClbfUJN"
      },
      "source": [
        "#training lstm/RNN model for train_2 (2020 pandemic)\n",
        "regressor_2 = Sequential()\n",
        "\n",
        "regressor_2.add(LSTM(units=50, return_sequences=True, input_shape=(x_train_2.shape[1],1)))\n",
        "regressor_2.add(Dropout(0.2))\n",
        "\n",
        "regressor_2.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_2.add(Dropout(0.2))\n",
        "\n",
        "regressor_2.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_2.add(Dropout(0.2))\n",
        "\n",
        "regressor_2.add(LSTM(units=50))\n",
        "regressor_2.add(Dropout(0.2))\n",
        "\n",
        "regressor_2.add(Dense(units=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyEmLTQ4fWDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172c2ee2-586f-4fb6-e0e7-6507333cc8a5"
      },
      "source": [
        "  #calculating loss using mean squared error and optimizing the model accordingly for train_2 (2020 pandemic)\n",
        "regressor_2.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "regressor_2.fit(x_train_2, y_train_2, epochs=5, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "9/9 [==============================] - 2s 181ms/step - loss: 0.0893\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 2s 205ms/step - loss: 0.0151\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 2s 214ms/step - loss: 0.0146\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 2s 221ms/step - loss: 0.0230\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 2s 229ms/step - loss: 0.0271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0784efef28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nra5bkNsKbMQ"
      },
      "source": [
        "test_1_new = df['High'][len(df['High'])-len(test_1)-100 : ].values\n",
        "test_1_new = test_1_new.reshape(-1,1)\n",
        "test_1_new = scaler.transform(test_1_new)\n",
        "test_2_new = df['High'][len(df['High'])-len(test_2)-100 : ].values\n",
        "test_2_new = test_2_new.reshape(-1,1)\n",
        "test_2_new = scaler.transform(test_2_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6oPYmh0d80m"
      },
      "source": [
        "  #inserting testing values according to their respective lists\n",
        "x_test_1 = list()\n",
        "x_test_2 = list()\n",
        "\n",
        "for i in range(100,len(test_1_new)):\n",
        "    x_test_1.append(test_1_new[i-100:i, 0])\n",
        "    \n",
        "x_test_1 = np.array(x_test_1)\n",
        "x_test_1 = np.reshape(x_test_1, (x_test_1.shape[0], x_test_1.shape[1], 1))\n",
        "\n",
        "for i in range(100,len(test_2_new)):\n",
        "    x_test_2.append(test_2_new[i-100:i, 0])\n",
        "    \n",
        "x_test_2 = np.array(x_test_2)\n",
        "x_test_2 = np.reshape(x_test_2, (x_test_2.shape[0], x_test_2.shape[1], 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnR8egRfd-GD"
      },
      "source": [
        "  #using the regressor for test after training lstm\n",
        "predicted_1 = regressor_1.predict(x_test_1)\n",
        "predicted_1 = scaler.inverse_transform(predicted_1)\n",
        "\n",
        "predicted_2 = regressor_2.predict(x_test_2)\n",
        "predicted_2 = scaler.inverse_transform(predicted_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kUQxLFm0Aif"
      },
      "source": [
        "Graphs display the stock's variation ,if the events hadn't occured in their respective time periods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkptDO3ue1gz"
      },
      "source": [
        "  #graphs description while displaying \n",
        "def plot_predictions(test, predicted, stock):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.plot(test, color=\"red\", label=\"real stock price\")\n",
        "    plt.plot(predicted, color=\"blue\", label=\"predicted stock price\")\n",
        "    plt.xlabel(\"time\")\n",
        "    plt.legend()\n",
        "    plt.savefig('drive/My Drive/data/results/'+stock+'.png')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdN4A3cSevaN"
      },
      "source": [
        "plot_predictions(test_1, predicted_1, 'BA_2008')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I40AhhUcggsJ"
      },
      "source": [
        "plot_predictions(test_2, predicted_2, 'BA_2020')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}